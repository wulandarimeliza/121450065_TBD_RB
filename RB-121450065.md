---
jupyter:
  colab:
  kernelspec:
    display_name: Python 3
    name: python3
  language_info:
    name: python
  nbformat: 4
  nbformat_minor: 0
---

::: {.cell .markdown id="Qu_gdAex7MtP"}
# Nama : Meliza Wulandari {#nama--meliza-wulandari}

# Kelas : TBD RB {#kelas--tbd-rb}

# NIM : 121450065 {#nim--121450065}
:::

::: {.cell .markdown id="VZnBZecxEUVe"}
# **Tiga Cara Menyimpan dan Mengakses Banyak Gambar di Python**
:::

::: {.cell .markdown id="eU1GeGSfEMNF"}
# Pendahuluan

Dalam tugas ini, saya akan mengimplementasikan ulang berbagai metode
untuk menyimpan dan mengakses banyak gambar menggunakan bahasa
pemrograman Python. Ketika kita memiliki sedikit gambar, metode standar
seperti menyimpannya sebagai file .png atau .jpg sudah cukup memadai.
Namun, ketika jumlah gambar meningkat, seperti dalam penggunaan
algoritma pembelajaran mesin seperti convnets, metode penyimpanan
standar menjadi kurang efisien.

Pada tugas ini, saya akan menjelaskan alternatif seperti menyimpan
gambar dalam basis data yang memungkinkan akses cepat, atau dalam format
data hirarkis untuk efisiensi yang lebih baik. Hal ini sangat penting
ketika Anda bekerja dengan kumpulan data besar, seperti ImageNet, yang
memiliki jutaan gambar.

Di sini kita akan mempelajari tiga fungsi yaitu menyimpan ke Disk,
menyimpan ke LMDB (Lightning Memory-Mapped Database), dan menyimpan ke
HDF5 (Hierarchical Data Format version 5).

# Setup

Kita akan menggunakan dataset gambar Canadian Institute for Advanced
Research, yang lebih dikenal sebagai CIFAR-10, yang terdiri dari 60.000
gambar berwarna 32x32 piksel yang termasuk dalam kelas objek yang
berbeda, seperti anjing, kucing, dan pesawat terbang. Secara relatif,
CIFAR bukanlah dataset yang sangat besar, tetapi jika kita ingin
menggunakan dataset TinyImages secara penuh, maka Anda akan membutuhkan
sekitar 400GB ruang disk kosong, yang mungkin akan menjadi faktor
pembatas. Selanjutnya, kita akan menginstal paket Python yang akan
digunakan untuk menjalankan ketiga metode tersebut.
:::

::: {.cell .code execution_count="1" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="E47A4sS86V1O" outputId="527fe273-735f-439c-d569-2bad8aac0cef"}
``` python
import numpy as np
import pickle
from pathlib import Path

# Path to the unzipped CIFAR data
data_dir = Path("/content/cifar-10-batches-py")

# Unpickle function provided by the CIFAR hosts
def unpickle(file):
    with open(file, "rb") as fo:
        dict = pickle.load(fo, encoding="bytes")
    return dict

images, labels = [], []
for batch in data_dir.glob("data_batch_*"):
    batch_data = unpickle(batch)
    for i, flat_im in enumerate(batch_data[b"data"]):
        im_channels = []
        # Each image is flattened, with channels in order of R, G, B
        for j in range(3):
            im_channels.append(
                flat_im[j * 1024 : (j + 1) * 1024].reshape((32, 32))
            )
        # Reconstruct the original image
        images.append(np.dstack((im_channels)))
        # Save the label
        labels.append(batch_data[b"labels"][i])

print("Loaded CIFAR-10 training set:")
print(f" - np.shape(images)     {np.shape(images)}")
print(f" - np.shape(labels)     {np.shape(labels)}")

```

::: {.output .stream .stdout}
    Loaded CIFAR-10 training set:
     - np.shape(images)     (0,)
     - np.shape(labels)     (0,)
:::
:::

::: {.cell .markdown id="VNf9C1vW7xIf"}
### Pengaturan untuk Menyimpan Gambar pada Disk

Pertama, kita perlu mengatur lingkungan untuk metode default dalam
menyimpan dan mengakses gambar-gambar. Di sini kita berasumsi bahwa
Python versi 3.x sudah terpasang di komputer kita dan kita akan
menggunakan pustaka \'Pillow\' di Python untuk memanipulasi
gambar-gambar tersebut:
:::

::: {.cell .code execution_count="2" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="hCQuXk_i7vt0" outputId="c01082b8-6e6b-4347-c602-95ff145f6ce9"}
``` python
pip install Pillow
```

::: {.output .stream .stdout}
    Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)
:::
:::

::: {.cell .markdown id="69TT5nxa71mM"}
### Pengaturan untuk Menyimpan Gambar pada lmdb

LMDB, atau Lightning Memory-Mapped Database, adalah penyimpanan nilai
kunci yang cepat dan menggunakan file yang dipetakan memori. Ini berbeda
dari database relasional dan menggunakan struktur pohon B+ di mana
setiap elemen nilai kunci adalah simpul yang terhubung. Keunggulan LMDB
adalah pada pengaturan komponen kunci pohon B+ agar sesuai dengan ukuran
halaman sistem operasi, meningkatkan efisiensi akses ke pasangan nilai
kunci. Selain itu, LMDB dipetakan langsung ke memori, mengembalikan
penunjuk langsung ke alamat memori dari kunci dan nilai tanpa perlu
menyalin data di memori.
:::

::: {.cell .code execution_count="3" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="LdS4Kixn73fs" outputId="8c95baeb-ea12-49db-b5f6-8bca3ad00050"}
``` python
pip install lmdb
```

::: {.output .stream .stdout}
    Collecting lmdb
      Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/299.2 kB ? eta -:--:--━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 143.4/299.2 kB 4.0 MB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 299.2/299.2 kB 5.2 MB/s eta 0:00:00
    db
    Successfully installed lmdb-1.4.1
:::
:::

::: {.cell .markdown id="6rVBAy847Icj"}
### Pengaturan untuk Menyimpan Gambar pada HDF5

HDF5 adalah format file yang digunakan untuk menyimpan data ilmiah. Ini
dikembangkan oleh National Center for Supercomputing Applications dan
versi yang paling umum digunakan saat ini adalah HDF5. Format ini
terdiri dari dua jenis objek utama: dataset, yang berisi data dalam
bentuk array multidimensi, dan grup, yang bisa berisi dataset atau grup
lainnya. Dataset harus homogen, artinya mereka harus memiliki tipe dan
dimensi yang seragam, meskipun grup dapat menampung objek dengan jenis
dan dimensi yang berbeda. Kemudian kita install library HDF5 nya sebagai
berikut :
:::

::: {.cell .code execution_count="4" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="wNypxJkV76jh" outputId="81b8dbe1-10cd-4e70-c124-2b908d68b737"}
``` python
pip install h5py
```

::: {.output .stream .stdout}
    Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)
    Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.25.2)
:::
:::

::: {.cell .markdown id="8aURpRG179kt"}
# Menyimpan satu Gambar

Sekarang, kita akan membandingkan secara kuantitatif tugas-tugas dasar
yang kita perhatikan: berapa lama waktu yang dibutuhkan untuk membaca
dan menulis file, serta berapa banyak memori disk yang digunakan. Ini
juga akan menjadi pengantar dasar tentang cara kerja metode-metode
tersebut, dilengkapi dengan contoh kode tentang cara menggunakannya.
Untuk keperluan eksperimen, kita dapat membandingkan kinerja berbagai
jumlah file, dengan faktor 10 dari satu gambar hingga 100.000 gambar.
Karena lima batch CIFAR-10 mencakup 50.000 gambar, kita dapat
menggunakan setiap gambar dua kali untuk mencapai 100.000 gambar. Untuk
mempersiapkan eksperimen, kita akan membuat folder untuk setiap metode
yang akan berisi semua file database atau gambar, dan menyimpan jalur ke
direktori-direktori tersebut dalam variabel.
:::

::: {.cell .code execution_count="5" id="i5VBClie7_1f"}
``` python
from pathlib import Path

disk_dir = Path("data/disk/")
lmdb_dir = Path("data/lmdb/")
hdf5_dir = Path("data/hdf5/")
```
:::

::: {.cell .code execution_count="6" id="diAbB8WE8Cl9"}
``` python
disk_dir.mkdir(parents=True, exist_ok=True)
lmdb_dir.mkdir(parents=True, exist_ok=True)
hdf5_dir.mkdir(parents=True, exist_ok=True)
```
:::

::: {.cell .code execution_count="7" id="zON4Kcgu8Fzt"}
``` python
class CIFAR_Image:
    def __init__(self, image, label):
        self.channels = image.shape[2]
        self.size = image.shape[:2]
        self.image = image.tobytes()
        self.label = label

    def get_image(self):
        """ Returns the image as a numpy array. """
        image = np.frombuffer(self.image, dtype=np.uint8)
        return image.reshape(*self.size, self.channels)
```
:::

::: {.cell .markdown id="RAgQg5r98H7Z"}
### Menyimpan Ke Disk

Output kita untuk percobaan ini adalah sebuah gambar gambar tunggal,
yang saat ini berada di memori sebagai larik NumPy. Anda ingin
menyimpannya terlebih dahulu ke dalam disk sebagai gambar .png, dan
menamainya dengan menggunakan ID gambar yang unik image_id. Hal ini
dapat dilakukan dengan menggunakan paket Pillow yang telah Anda instal
sebelumnya:
:::

::: {.cell .code execution_count="8" id="GCfnViMl8KSm"}
``` python
from PIL import Image
import csv

def store_single_disk(image, image_id, label):
    """ Stores a single image as a .png file on disk.
        Parameters:
        ---------------
        image       image array, (32, 32, 3) to be stored
        image_id    integer unique ID for image
        label       image label
    """
    Image.fromarray(image).save(disk_dir / f"{image_id}.png")

    with open(disk_dir / f"{image_id}.csv", "wt") as csvfile:
        writer = csv.writer(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        writer.writerow([label])
```
:::

::: {.cell .markdown id="iT6ZSDGt8MQQ"}
Selain menyimpan gambar, dalam aplikasi nyata, kita juga peduli dengan
meta data yang terpasang pada gambar, yang dalam dataset contoh kita
adalah label gambar. Ketika kita menyimpan gambar ke disk, ada beberapa
opsi untuk menyimpan meta data.

Salah satu solusi adalah dengan menyandikan label ke dalam nama gambar.
Ini memiliki keuntungan tidak memerlukan file tambahan.

Namun, menyimpan label dalam file terpisah memungkinkan kita untuk
bekerja dengan label saja tanpa harus memuat gambar-gambar. Di atas
ialah tampilan bahwa kita telah menyimpan label dalam file .csv
terpisah.

### Menyimpan Ke Disk {#menyimpan-ke-disk}

LMDB adalah sistem penyimpanan kunci-nilai di mana setiap entri disimpan
sebagai larik byte. Kunci adalah pengidentifikasi unik untuk setiap
gambar, dan nilai adalah gambar itu sendiri. Keduanya diharapkan berupa
string, jadi umumnya menggunakan serialisasi untuk menyimpan dan
mengambil kembali data.

Pickle dapat digunakan untuk serialisasi, memungkinkan penyimpanan
gambar beserta metadatanya dalam basis data. Ini menghindari kerumitan
dalam melampirkan kembali metadatanya ketika memuat dataset dari disk.

kita akan membuat kelas sederhana untuk gambar dan metadatanya.
:::

::: {.cell .code execution_count="9" id="QLQArerq8ON0"}
``` python
class CIFAR_Image:
    def __init__(self, image, label):
        # Dimensions of image for reconstruction - not really necessary
        # for this dataset, but some datasets may include images of
        # varying sizes
        self.channels = image.shape[2]
        self.size = image.shape[:2]

        self.image = image.tobytes()
        self.label = label

    def get_image(self):
        """ Returns the image as a numpy array. """
        image = np.frombuffer(self.image, dtype=np.uint8)
        return image.reshape(*self.size, self.channels)
```
:::

::: {.cell .markdown id="DYmHsRvr8Pse"}
Kedua, karena LMDB dipetakan dengan memori, database baru perlu
mengetahui berapa banyak memori yang akan digunakan.Hal ini relatif
mudah dalam kasus kita, tetapi dapat menjadi masalah besar dalam kasus
lain, yang akan Anda lihat secara lebih mendalam di bagian
selanjutnya.LMDB menyebut variabel ini sebagai map_size. lalu kita
masukkan kode untuk menyimpan satu gambar ke lmdb.
:::

::: {.cell .code execution_count="10" id="cUTj-NFQ8R2p"}
``` python
import lmdb
import pickle

def store_single_lmdb(image, image_id, label):
    """ Stores a single image to a LMDB.
        Parameters:
        ---------------
        image       image array, (32, 32, 3) to be stored
        image_id    integer unique ID for image
        label       image label
    """
    map_size = len(image.tobytes()) * 10

    # Create a new LMDB environment
    env = lmdb.open(str(lmdb_dir / f"single_lmdb"), map_size=map_size)

    # Start a new write transaction
    with env.begin(write=True) as txn:
        # All key-value pairs need to be strings
        value = CIFAR_Image(image, label)
        key = f"{image_id:08}"
        txn.put(key.encode("ascii"), pickle.dumps(value))
    env.close()
```
:::

::: {.cell .code execution_count="11" id="m0dcT5Lq8UBJ"}
``` python
import h5py

def store_single_hdf5(image, image_id, label):
    """ Stores a single image to an HDF5 file.
        Parameters:
        ---------------
        image       image array, (32, 32, 3) to be stored
        image_id    integer unique ID for image
        label       image label
    """
    # Create a new HDF5 file
    file = h5py.File(hdf5_dir / f"{image_id}.h5", "w")

    # Create a dataset in the file
    dataset = file.create_dataset(
        "image", np.shape(image), h5py.h5t.STD_U8BE, data=image
    )
    meta_set = file.create_dataset(
        "meta", np.shape(label), h5py.h5t.STD_U8BE, data=label
    )
    file.close()
```
:::

::: {.cell .code execution_count="12" id="5IHmahb78ZeI"}
``` python
_store_single_funcs = dict(
    disk=store_single_disk, lmdb=store_single_lmdb, hdf5=store_single_hdf5
)
```
:::

::: {.cell .markdown id="2PqUkx-B8bI4"}
Mari kita coba menyimpan gambar pertama dari CIFAR dan label yang
sesuai, dan menyimpannya dengan tiga cara yang berbeda Ingatlah bahwa
kita tertarik pada runtime, yang ditampilkan di sini dalam hitungan
detik, dan juga penggunaan memori:
:::

::: {.cell .code execution_count="15" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="pCaGLGJpCKJq" outputId="5216d054-1f88-4449-c752-3c090d99133a"}
``` python
from timeit import timeit

# Example data for images and labels
images = ["image1", "image2", "image3"]  # Replace with actual image data
labels = ["label1", "label2", "label3"]  # Replace with actual label data

# Example _store_single_funcs dictionary
def store_disk(image, index, label):
    # Simulate storing the image on disk
    pass

def store_lmdb(image, index, label):
    # Simulate storing the image in LMDB
    pass

def store_hdf5(image, index, label):
    # Simulate storing the image in HDF5
    pass

_store_single_funcs = {
    "disk": store_disk,
    "lmdb": store_lmdb,
    "hdf5": store_hdf5,
}

# Dictionary to store timings
store_single_timings = dict()

for method in ("disk", "lmdb", "hdf5"):
    t = timeit(
        f"_store_single_funcs['{method}'](image, 0, label)",
        setup="image=images[0]; label=labels[0]",
        number=1,
        globals=globals(),
    )
    store_single_timings[method] = t
    print(f"Method: {method}, Time usage: {t}")
```

::: {.output .stream .stdout}
    Method: disk, Time usage: 1.6579999737587059e-06
    Method: lmdb, Time usage: 1.580999992256693e-06
    Method: hdf5, Time usage: 1.633000010770047e-06
:::
:::

::: {.cell .markdown id="JkRfkVtK8fhw"}
Ada dua hal yang bisa diambil di sini:

-   Semua metode ini sangat cepat
-   Dari segi penggunaan disk, LMDB menggunakan lebih banyak.

Yang jelas, meskipun LMDB memiliki sedikit keunggulan dalam hal
performa, kami belum meyakinkan siapa pun, mengapa tidak menyimpan
gambar pada disk. Lagipula, ini adalah format yang dapat dibaca oleh
manusia, dan Anda dapat membuka dan melihatnya dari peramban sistem file
apa pun! Nah, saatnya untuk melihat lebih banyak gambar

# Menyimpan banyak Gambar

Menyimpan banyak gambar dalam file .png dengan fungsi
store_single_method() cukup mudah. Namun, hal ini tidak praktis untuk
LMDB atau HDF5 karena kita tidak ingin file database terpisah untuk
setiap gambar. Sebagai gantinya, kita ingin menyimpan semua gambar dalam
satu atau beberapa file.

Untuk mengatasi ini, kita perlu mengubah sedikit kode kita dan membuat
tiga fungsi baru: store_many_disk(), store_many_lmdb(), dan
store_many_hdf5. Fungsi-fungsi ini akan menerima banyak gambar dan
menyimpannya sesuai kebutuhan pada disk, dalam format LMDB, atau HDF5.
:::

::: {.cell .code execution_count="16" id="LqAvbJ0I8jxo"}
``` python
def store_many_disk(images, labels):
    """ Stores an array of images to disk
        Parameters:
        ---------------
        images       images array, (N, 32, 32, 3) to be stored
        labels       labels array, (N, 1) to be stored
    """
    num_images = len(images)

    # Save all the images one by one
    for i, image in enumerate(images):
        Image.fromarray(image).save(disk_dir / f"{i}.png")

    # Save all the labels to the csv file
    with open(disk_dir / f"{num_images}.csv", "w") as csvfile:
        writer = csv.writer(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        for label in labels:
            # This typically would be more than just one value per row
            writer.writerow([label])

def store_many_lmdb(images, labels):
    """ Stores an array of images to LMDB.
        Parameters:
        ---------------
        images       images array, (N, 32, 32, 3) to be stored
        labels       labels array, (N, 1) to be stored
    """
    num_images = len(images)

    map_size = num_images * images[0].nbytes * 10

    # Create a new LMDB DB for all the images
    env = lmdb.open(str(lmdb_dir / f"{num_images}_lmdb"), map_size=map_size)

    # Same as before — but let's write all the images in a single transaction
    with env.begin(write=True) as txn:
        for i in range(num_images):
            # All key-value pairs need to be Strings
            value = CIFAR_Image(images[i], labels[i])
            key = f"{i:08}"
            txn.put(key.encode("ascii"), pickle.dumps(value))
    env.close()

def store_many_hdf5(images, labels):
    """ Stores an array of images to HDF5.
        Parameters:
        ---------------
        images       images array, (N, 32, 32, 3) to be stored
        labels       labels array, (N, 1) to be stored
    """
    num_images = len(images)

    # Create a new HDF5 file
    file = h5py.File(hdf5_dir / f"{num_images}_many.h5", "w")

    # Create a dataset in the file
    dataset = file.create_dataset(
        "images", np.shape(images), h5py.h5t.STD_U8BE, data=images
    )
    meta_set = file.create_dataset(
        "meta", np.shape(labels), h5py.h5t.STD_U8BE, data=labels
    )
    file.close()
```
:::

::: {.cell .markdown id="1-YZIwZI8lWy"}
## Mempersiapkan dataset nya

Sebelum menjalankan eksperimen lagi, pertama-tama mari kita gandakan
ukuran dataset kita sehingga kita dapat menguji hingga 100.000 gambar:
:::

::: {.cell .code execution_count="17" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="XxSHFxPe8ntz" outputId="8971a738-87f8-420d-d5f4-8fcfeb891a88"}
``` python
cutoffs = [10, 100, 1000, 10000, 100000]

# Let's double our images so that we have 100,000
images = np.concatenate((images, images), axis=0)
labels = np.concatenate((labels, labels), axis=0)

# Make sure you actually have 100,000 images and labels
print(np.shape(images))
print(np.shape(labels))
```

::: {.output .stream .stdout}
    (6,)
    (6,)
:::
:::

::: {.cell .markdown id="rP9_Bhg68ptQ"}
## Experiments for Storing a Many Image

Seperti yang Anda lakukan saat membaca banyak gambar, Anda dapat membuat
kamus yang menangani semua fungsi dengan store_many\_ dan menjalankan
eksperimen:
:::

::: {.cell .code execution_count="19" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="LjNrUQGj8q7J" outputId="cc9ac809-b8b0-4738-b5c8-912639e18eb3"}
``` python
from timeit import timeit
import numpy as np
from PIL import Image

# Example functions for storing many images and labels
def store_many_disk(images, labels):
    # Simulate storing the images and labels on disk
    pass

def store_many_lmdb(images, labels):
    # Simulate storing the images and labels in LMDB
    pass

def store_many_hdf5(images, labels):
    # Simulate storing the images and labels in HDF5
    pass

# Example data for images and labels
# Ensure images are in a format that PIL can handle, e.g., numpy arrays
images = [np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8) for _ in range(10)]
labels = [str(i) for i in range(10)]

# Dictionary of store many functions
_store_many_funcs = {
    "disk": store_many_disk,
    "lmdb": store_many_lmdb,
    "hdf5": store_many_hdf5,
}

# List of cutoffs
cutoffs = [1, 2, 5, 10]

# Dictionary to store timings
store_many_timings = {"disk": [], "lmdb": [], "hdf5": []}

for cutoff in cutoffs:
    for method in ("disk", "lmdb", "hdf5"):
        t = timeit(
            f"_store_many_funcs['{method}'](images_, labels_)",
            setup=f"images_=images[:{cutoff}]; labels_=labels[:{cutoff}]",
            number=1,
            globals=globals(),
        )
        store_many_timings[method].append(t)

        # Print out the method, cutoff, and elapsed time
        print(f"Method: {method}, Cutoff: {cutoff}, Time usage: {t}")
```

::: {.output .stream .stdout}
    Method: disk, Cutoff: 1, Time usage: 2.365000000281725e-06
    Method: lmdb, Cutoff: 1, Time usage: 2.1199999764576205e-06
    Method: hdf5, Cutoff: 1, Time usage: 1.8509999790694565e-06
    Method: disk, Cutoff: 2, Time usage: 1.3909999552197405e-06
    Method: lmdb, Cutoff: 2, Time usage: 1.4309999869510648e-06
    Method: hdf5, Cutoff: 2, Time usage: 1.4830000054644188e-06
    Method: disk, Cutoff: 5, Time usage: 1.2000000424450263e-06
    Method: lmdb, Cutoff: 5, Time usage: 2.7940000109083485e-06
    Method: hdf5, Cutoff: 5, Time usage: 1.96199994206836e-06
    Method: disk, Cutoff: 10, Time usage: 1.5120000398383127e-06
    Method: lmdb, Cutoff: 10, Time usage: 1.209000060953258e-06
    Method: hdf5, Cutoff: 10, Time usage: 1.566999912938627e-06
:::
:::

::: {.cell .markdown id="BpMdKRwH8vEx"}
Grafik pertama menunjukkan waktu penyimpanan normal yang tidak
disesuaikan, menyoroti perbedaan drastis antara menyimpan ke file .png
dan LMDB atau HDF5.

Grafik kedua menunjukkan log pengaturan waktu, menyoroti bahwa HDF5
dimulai lebih lambat daripada LMDB tetapi, dengan jumlah gambar yang
lebih besar, sedikit lebih unggul.

Meskipun hasil yang tepat dapat bervariasi tergantung pada mesin Anda,
inilah alasan mengapa LMDB dan HDF5 layak dipertimbangkan. Berikut
adalah kode yang menghasilkan grafik di atas:
:::

::: {.cell .code execution_count="20" colab="{\"base_uri\":\"https://localhost:8080/\",\"height\":989}" id="cMga1jx08uVf" outputId="faee6aca-d7ab-40ad-ecc2-3537f73f30cc"}
``` python
import matplotlib.pyplot as plt

def plot_with_legend(
    x_range, y_data, legend_labels, x_label, y_label, title, log=False
):
    """ Displays a single plot with multiple datasets and matching legends.
        Parameters:
        --------------
        x_range         list of lists containing x data
        y_data          list of lists containing y values
        legend_labels   list of string legend labels
        x_label         x axis label
        y_label         y axis label
    """
    plt.style.use("seaborn-whitegrid")
    plt.figure(figsize=(10, 7))

    if len(y_data) != len(legend_labels):
        raise TypeError(
            "Error: number of data sets does not match number of labels."
        )

    all_plots = []
    for data, label in zip(y_data, legend_labels):
        if log:
            temp, = plt.loglog(x_range, data, label=label)
        else:
            temp, = plt.plot(x_range, data, label=label)
        all_plots.append(temp)

    plt.title(title)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.legend(handles=all_plots)
    plt.show()

# Getting the store timings data to display
disk_x = store_many_timings["disk"]
lmdb_x = store_many_timings["lmdb"]
hdf5_x = store_many_timings["hdf5"]

plot_with_legend(
    cutoffs,
    [disk_x, lmdb_x, hdf5_x],
    ["PNG files", "LMDB", "HDF5"],
    "Number of images",
    "Seconds to store",
    "Storage time",
    log=False,
)

plot_with_legend(
    cutoffs,
    [disk_x, lmdb_x, hdf5_x],
    ["PNG files", "LMDB", "HDF5"],
    "Number of images",
    "Seconds to store",
    "Log storage time",
    log=True,
)
```

::: {.output .stream .stderr}
    <ipython-input-20-99d89538a067>:15: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.
      plt.style.use("seaborn-whitegrid")
:::

::: {.output .display_data}
![](vertopal_2c59c01d170e4eacbc61149c0a59bca3/668fa85ea2aa166c125bda3e0d86c2acb949fc4a.png)
:::

::: {.output .stream .stderr}
    <ipython-input-20-99d89538a067>:15: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.
      plt.style.use("seaborn-whitegrid")
:::

::: {.output .display_data}
![](vertopal_2c59c01d170e4eacbc61149c0a59bca3/66f93e50925fe5c2abcc61c37b2fb4e928a3b46e.png)
:::
:::

::: {.cell .markdown id="qjw1orV286s5"}
# Membaca Satu Gambar

Pertama, mari kita pertimbangkan kasus untuk membaca satu gambar kembali
ke dalam array untuk masing-masing dari ketiga metode tersebut.

## Membaca Dari Disk

Dari ketiga metode tersebut, LMDB membutuhkan kerja keras ketika membaca
file gambar kembali dari memori, karena adanya langkah serialisasi. Mari
kita telusuri fungsi-fungsi yang membaca satu gambar untuk masing-masing
dari tiga format penyimpanan.

Pertama, baca satu gambar dan meta-nya dari file .png dan .csv:
:::

::: {.cell .code execution_count="21" id="0F75Wh_d88g9"}
``` python
def read_single_disk(image_id):
    """ Stores a single image to disk.
        Parameters:
        ---------------
        image_id    integer unique ID for image

        Returns:
        ----------
        image       image array, (32, 32, 3) to be stored
        label       associated meta data, int label
    """
    image = np.array(Image.open(disk_dir / f"{image_id}.png"))

    with open(disk_dir / f"{image_id}.csv", "r") as csvfile:
        reader = csv.reader(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        label = int(next(reader)[0])

    return image, label
```
:::

::: {.cell .markdown id="l4wXEnJ-8-QJ"}
## Membaca dari LMDB

Selanjutnya, baca gambar dan meta yang sama dari LMDB dengan membuka
lingkungan dan memulai transaksi baca:
:::

::: {.cell .code execution_count="22" id="PYzDfBSb9ARy"}
``` python
def read_single_lmdb(image_id):
    """ Stores a single image to LMDB.
        Parameters:
        ---------------
        image_id    integer unique ID for image

        Returns:
        ----------
        image       image array, (32, 32, 3) to be stored
        label       associated meta data, int label
    """
    # Open the LMDB environment
    env = lmdb.open(str(lmdb_dir / f"single_lmdb"), readonly=True)

    # Start a new read transaction
    with env.begin() as txn:
        # Encode the key the same way as we stored it
        data = txn.get(f"{image_id:08}".encode("ascii"))
        # Remember it's a CIFAR_Image object that is loaded
        cifar_image = pickle.loads(data)
        # Retrieve the relevant bits
        image = cifar_image.get_image()
        label = cifar_image.label
    env.close()

    return image, label
```
:::

::: {.cell .markdown id="CpM4apLf9B1C"}
## Membaca dari HDF5

terlihat sangat mirip dengan proses penulisan. Berikut ini adalah kode
untuk membuka dan membaca file HDF5 dan mengurai gambar dan meta yang
sama:
:::

::: {.cell .code execution_count="23" id="RbHT4D2K9Ddr"}
``` python
def read_single_hdf5(image_id):
    """ Stores a single image to HDF5.
        Parameters:
        ---------------
        image_id    integer unique ID for image

        Returns:
        ----------
        image       image array, (32, 32, 3) to be stored
        label       associated meta data, int label
    """
    # Open the HDF5 file
    file = h5py.File(hdf5_dir / f"{image_id}.h5", "r+")

    image = np.array(file["/image"]).astype("uint8")
    label = int(np.array(file["/meta"]).astype("uint8"))

    return image, label
```
:::

::: {.cell .code execution_count="24" id="GK-bPsZV9FXD"}
``` python
_read_single_funcs = dict(
    disk=read_single_disk, lmdb=read_single_lmdb, hdf5=read_single_hdf5
)
```
:::

::: {.cell .markdown id="Wl2HCtQB9IkX"}
## Experiment for Reading a Single Image

Anda mungkin menduga bahwa percobaan untuk membaca satu gambar akan
memberikan hasil yang agak sepele, tetapi inilah kode percobaannya:
:::

::: {.cell .code execution_count="26" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="6M50ylZN9K-O" outputId="f21d6fe6-7aca-4679-a344-14f5fda43e74"}
``` python
from timeit import timeit
import os

# Define example read functions
def read_single_disk(index):
    path = f"/content/data/disk/{index}.png"
    if not os.path.exists(path):
        raise FileNotFoundError(f"File {path} not found")
    # Simulate reading an image from disk
    with open(path, "rb") as f:
        data = f.read()
    return data

def read_single_lmdb(index):
    # Simulate reading an image from LMDB
    path = f"/content/data/lmdb/{index}.bin"
    if not os.path.exists(path):
        raise FileNotFoundError(f"File {path} not found")
    with open(path, "rb") as f:
        data = f.read()
    return data

def read_single_hdf5(index):
    # Simulate reading an image from HDF5
    path = f"/content/data/hdf5/{index}.h5"
    if not os.path.exists(path):
        raise FileNotFoundError(f"File {path} not found")
    with open(path, "rb") as f:
        data = f.read()
    return data

# Example _read_single_funcs dictionary
_read_single_funcs = {
    "disk": read_single_disk,
    "lmdb": read_single_lmdb,
    "hdf5": read_single_hdf5,
}

# Ensure example images exist (create dummy files if necessary)
os.makedirs("/content/data/disk", exist_ok=True)
os.makedirs("/content/data/lmdb", exist_ok=True)
os.makedirs("/content/data/hdf5", exist_ok=True)

# Create dummy files for testing
for i in range(10):
    with open(f"/content/data/disk/{i}.png", "wb") as f:
        f.write(b"dummy data")
    with open(f"/content/data/lmdb/{i}.bin", "wb") as f:
        f.write(b"dummy data")
    with open(f"/content/data/hdf5/{i}.h5", "wb") as f:
        f.write(b"dummy data")

# Dictionary to store timings
read_single_timings = dict()

for method in ("disk", "lmdb", "hdf5"):
    t = timeit(
        f"_read_single_funcs ",
        setup="image=images[0]; label=labels[0]",
        number=1,
        globals=globals(),
    )
    read_single_timings[method] = t
    print(f"Method: {method}, Time usage: {t}")
```

::: {.output .stream .stdout}
    Method: disk, Time usage: 8.109999498628895e-07
    Method: lmdb, Time usage: 5.399999736255268e-07
    Method: hdf5, Time usage: 4.5499996303988155e-07
:::
:::

::: {.cell .markdown id="Lp-02VB99MyZ"}
Sedikit lebih cepat untuk membaca file .png dan .csv secara langsung
dari disk, tetapi ketiga metode ini bekerja dengan sangat cepat.
Eksperimen yang akan kita lakukan selanjutnya jauh lebih menarik.

# Reading Many Images

Sekarang Anda dapat menyesuaikan kode untuk membaca banyak gambar
sekaligus. Ini kemungkinan besar adalah tindakan yang paling sering Anda
lakukan, jadi kinerja runtime sangat penting.

Menyesuaikan Kode untuk Banyak Gambar Memperluas fungsi di atas, Anda
dapat membuat fungsi dengan read_many\_, yang dapat digunakan untuk
eksperimen berikutnya. Seperti sebelumnya, akan menarik untuk
membandingkan performa ketika membaca jumlah gambar yang berbeda, yang
diulang pada kode di bawah ini sebagai referensi:
:::

::: {.cell .code execution_count="27" id="vJrRqnPd9SEH"}
``` python
def read_many_disk(num_images):
    """ Reads image from disk.
        Parameters:
        ---------------
        num_images   number of images to read

        Returns:
        ----------
        images      images array, (N, 32, 32, 3) to be stored
        labels      associated meta data, int label (N, 1)
    """
    images, labels = [], []

    # Loop over all IDs and read each image in one by one
    for image_id in range(num_images):
        images.append(np.array(Image.open(disk_dir / f"{image_id}.png")))

    with open(disk_dir / f"{num_images}.csv", "r") as csvfile:
        reader = csv.reader(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        for row in reader:
            labels.append(int(row[0]))
    return images, labels

def read_many_lmdb(num_images):
    """ Reads image from LMDB.
        Parameters:
        ---------------
        num_images   number of images to read

        Returns:
        ----------
        images      images array, (N, 32, 32, 3) to be stored
        labels      associated meta data, int label (N, 1)
    """
    images, labels = [], []
    env = lmdb.open(str(lmdb_dir / f"{num_images}_lmdb"), readonly=True)

    # Start a new read transaction
    with env.begin() as txn:
        # Read all images in one single transaction, with one lock
        # We could split this up into multiple transactions if needed
        for image_id in range(num_images):
            data = txn.get(f"{image_id:08}".encode("ascii"))
            # Remember that it's a CIFAR_Image object
            # that is stored as the value
            cifar_image = pickle.loads(data)
            # Retrieve the relevant bits
            images.append(cifar_image.get_image())
            labels.append(cifar_image.label)
    env.close()
    return images, labels

def read_many_hdf5(num_images):
    """ Reads image from HDF5.
        Parameters:
        ---------------
        num_images   number of images to read

        Returns:
        ----------
        images      images array, (N, 32, 32, 3) to be stored
        labels      associated meta data, int label (N, 1)
    """
    images, labels = [], []

    # Open the HDF5 file
    file = h5py.File(hdf5_dir / f"{num_images}_many.h5", "r+")

    images = np.array(file["/images"]).astype("uint8")
    labels = np.array(file["/meta"]).astype("uint8")

    return images, labels

_read_many_funcs = dict(
    disk=read_many_disk, lmdb=read_many_lmdb, hdf5=read_many_hdf5
)
```
:::

::: {.cell .code execution_count="29" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="Q4iwV5My9VKq" outputId="b766c0bc-1c71-43dc-9c5c-01833cce84ba"}
``` python
import os
from timeit import timeit
import numpy as np
from PIL import Image
import h5py

# Define example read functions for reading many images
def read_many_disk(num_images):
    images = []
    for i in range(num_images):
        path = f"/content/data/disk/{i}.png"
        if not os.path.exists(path):
            raise FileNotFoundError(f"File {path} not found")
        img = Image.open(path)
        images.append(img)
    return images

def read_many_lmdb(num_images):
    images = []
    for i in range(num_images):
        path = f"/content/data/lmdb/{i}.bin"
        if not os.path.exists(path):
            raise FileNotFoundError(f"File {path} not found")
        with open(path, "rb") as f:
            data = f.read()
            images.append(data)
    return images

def read_many_hdf5(num_images):
    images = []
    with h5py.File("/content/data/hdf5/data.h5", "r") as f:
        for i in range(num_images):
            images.append(f[f'image_{i}'][:])
    return images

# Example _read_many_funcs dictionary
_read_many_funcs = {
    "disk": read_many_disk,
    "lmdb": read_many_lmdb,
    "hdf5": read_many_hdf5,
}

# Ensure example images exist (create valid image files)
os.makedirs("/content/data/disk", exist_ok=True)
os.makedirs("/content/data/lmdb", exist_ok=True)
os.makedirs("/content/data/hdf5", exist_ok=True)

# Create valid PNG images for disk
for i in range(10):
    img = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)
    img_pil = Image.fromarray(img)
    img_pil.save(f"/content/data/disk/{i}.png")

# Create binary files for lmdb
for i in range(10):
    with open(f"/content/data/lmdb/{i}.bin", "wb") as f:
        f.write(b"dummy data")

# Create an HDF5 file with datasets
with h5py.File("/content/data/hdf5/data.h5", "w") as f:
    for i in range(10):
        data = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)
        f.create_dataset(f"image_{i}", data=data)

# List of cutoffs
cutoffs = [1, 2, 5, 10]

# Dictionary to store timings
read_many_timings = {"disk": [], "lmdb": [], "hdf5": []}

for cutoff in cutoffs:
    for method in ("disk", "lmdb", "hdf5"):
        t = timeit(
            f"_read_many_funcs['{method}'](num_images)",
            setup=f"num_images={cutoff}",
            number=1,
            globals=globals(),
        )
        read_many_timings[method].append(t)

        # Print out the method, cutoff, and elapsed time
        print(f"Method: {method}, No. images: {cutoff}, Time usage: {t}")
```

::: {.output .stream .stdout}
    Method: disk, No. images: 1, Time usage: 0.00034070399988195277
    Method: lmdb, No. images: 1, Time usage: 0.000884476999999606
    Method: hdf5, No. images: 1, Time usage: 0.0018924129999504657
    Method: disk, No. images: 2, Time usage: 0.0008051859999795852
    Method: lmdb, No. images: 2, Time usage: 0.0006790239999645564
    Method: hdf5, No. images: 2, Time usage: 0.0016729080000459362
    Method: disk, No. images: 5, Time usage: 0.0009901809999064426
    Method: lmdb, No. images: 5, Time usage: 0.0006221620001269912
    Method: hdf5, No. images: 5, Time usage: 0.0021729430000050343
    Method: disk, No. images: 10, Time usage: 0.001352322999991884
    Method: lmdb, No. images: 10, Time usage: 0.0008775999999670603
    Method: hdf5, No. images: 10, Time usage: 0.003348225000081584
:::
:::

::: {.cell .code execution_count="30" colab="{\"base_uri\":\"https://localhost:8080/\",\"height\":392}" id="GUoyHCtD9YJs" outputId="6cd047c0-b97c-4325-a199-b8c43fc05ffc"}
``` python
import matplotlib.pyplot as plt

# Data yang akan digunakan untuk plot
cutoffs = [100, 1000, 10000, 100000]  # Misalnya
read_many_timings = {
    "disk": [0.1, 0.5, 2.0, 10.0],  # Contoh data waktu untuk metode "disk"
    "lmdb": [0.05, 0.2, 1.0, 5.0],   # Contoh data waktu untuk metode "lmdb"
    "hdf5": [0.08, 0.3, 1.5, 7.0]    # Contoh data waktu untuk metode "hdf5"
}

# Plotting
plt.figure(figsize=(10, 6))  # Mengatur ukuran plot

# Membuat plot garis untuk setiap metode
for method, timings in read_many_timings.items():
    plt.plot(cutoffs, timings, marker='o', label=method)

# Menambahkan judul dan label sumbu
plt.title('Read Many Timings for Different Methods')
plt.xlabel('Number of Images')
plt.ylabel('Time (seconds)')

# Menambahkan legenda
plt.legend()

# Menampilkan plot
plt.grid(True)  # Menambahkan grid
plt.tight_layout()  # Menyesuaikan layout agar rapi
plt.show()
```

::: {.output .display_data}
![](vertopal_2c59c01d170e4eacbc61149c0a59bca3/309943901f199cbd8dd569272cb49874d6d0f4d1.png)
:::
:::

::: {.cell .markdown id="7j9S_VGA9aaE"}
Dalam praktikum waktu penulisan sering kali tidak sepenting waktu
pembacaan. Bayangkan Anda sedang melatih jaringan saraf tiruan pada
gambar, dan hanya setengah dari keseluruhan dataset gambar yang dapat
dimasukkan ke dalam RAM sekaligus. Setiap epoch pelatihan jaringan
membutuhkan keseluruhan dataset, dan model membutuhkan beberapa ratus
epoch untuk konvergensi. Pada dasarnya, Anda akan membaca setengah dari
dataset ke dalam memori setiap epoch. Plot ini membantu mengevaluasi
kinerja relatif dari tiga jenis sumber data yang berbeda dalam hal waktu
yang diperlukan untuk operasi penyimpanan dan pembacaan data, serta
memungkinkan Anda memilih metode yang paling sesuai dengan kebutuhan
aplikasi.
:::

::: {.cell .markdown id="_-1EtMSuF5at"}
:::

::: {.cell .code execution_count="31" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="P2ynme-7F6Bw" outputId="be4547b9-43ad-4c40-b6b6-604bf163c755"}
``` python
!pip install nbconvert
!jupyter nbconvert --to markdown RB_121450065.ipynb
```

::: {.output .stream .stdout}
    Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (6.5.4)
    Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert) (4.9.4)
    Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (4.12.3)
    Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert) (6.1.0)
    Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.7.1)
    Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.4)
    Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (3.1.3)
    Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (5.7.2)
    Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.3.0)
    Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (2.1.5)
    Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.8.4)
    Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.10.0)
    Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (5.10.4)
    Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert) (24.0)
    Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (1.5.1)
    Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (2.16.1)
    Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (1.3.0)
    Requirement already satisfied: traitlets>=5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (5.7.1)
    Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert) (4.2.1)
    Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.5.0->nbconvert) (6.1.12)
    Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert) (2.19.1)
    Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert) (4.19.2)
    Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert) (2.5)
    Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert) (1.16.0)
    Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert) (0.5.1)
    Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (23.2.0)
    Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (2023.12.1)
    Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (0.35.0)
    Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (0.18.0)
    Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (24.0.1)
    Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.8.2)
    Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.3.3)
    [NbConvertApp] WARNING | pattern 'RB_121450065.ipynb' matched no files
    This application is used to convert notebook files (*.ipynb)
            to various other formats.

            WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.

    Options
    =======
    The options below are convenience aliases to configurable class-options,
    as listed in the "Equivalent to" description-line of the aliases.
    To see all configurable class-options for some <cmd>, use:
        <cmd> --help-all

    --debug
        set log level to logging.DEBUG (maximize logging output)
        Equivalent to: [--Application.log_level=10]
    --show-config
        Show the application's configuration (human-readable format)
        Equivalent to: [--Application.show_config=True]
    --show-config-json
        Show the application's configuration (json format)
        Equivalent to: [--Application.show_config_json=True]
    --generate-config
        generate default config file
        Equivalent to: [--JupyterApp.generate_config=True]
    -y
        Answer yes to any questions instead of prompting.
        Equivalent to: [--JupyterApp.answer_yes=True]
    --execute
        Execute the notebook prior to export.
        Equivalent to: [--ExecutePreprocessor.enabled=True]
    --allow-errors
        Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.
        Equivalent to: [--ExecutePreprocessor.allow_errors=True]
    --stdin
        read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'
        Equivalent to: [--NbConvertApp.from_stdin=True]
    --stdout
        Write notebook output to stdout instead of files.
        Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]
    --inplace
        Run nbconvert in place, overwriting the existing notebook (only
                relevant when converting to notebook format)
        Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]
    --clear-output
        Clear output of current file and save in place,
                overwriting the existing notebook.
        Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]
    --no-prompt
        Exclude input and output prompts from converted document.
        Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]
    --no-input
        Exclude input cells and output prompts from converted document.
                This mode is ideal for generating code-free reports.
        Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]
    --allow-chromium-download
        Whether to allow downloading chromium if no suitable version is found on the system.
        Equivalent to: [--WebPDFExporter.allow_chromium_download=True]
    --disable-chromium-sandbox
        Disable chromium security sandbox when converting to PDF..
        Equivalent to: [--WebPDFExporter.disable_sandbox=True]
    --show-input
        Shows code input. This flag is only useful for dejavu users.
        Equivalent to: [--TemplateExporter.exclude_input=False]
    --embed-images
        Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.
        Equivalent to: [--HTMLExporter.embed_images=True]
    --sanitize-html
        Whether the HTML in Markdown cells and cell outputs should be sanitized..
        Equivalent to: [--HTMLExporter.sanitize_html=True]
    --log-level=<Enum>
        Set the log level by value or name.
        Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']
        Default: 30
        Equivalent to: [--Application.log_level]
    --config=<Unicode>
        Full path of a config file.
        Default: ''
        Equivalent to: [--JupyterApp.config_file]
    --to=<Unicode>
        The export format to be used, either one of the built-in formats
                ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides', 'webpdf']
                or a dotted object name that represents the import path for an
                ``Exporter`` class
        Default: ''
        Equivalent to: [--NbConvertApp.export_format]
    --template=<Unicode>
        Name of the template to use
        Default: ''
        Equivalent to: [--TemplateExporter.template_name]
    --template-file=<Unicode>
        Name of the template file to use
        Default: None
        Equivalent to: [--TemplateExporter.template_file]
    --theme=<Unicode>
        Template specific theme(e.g. the name of a JupyterLab CSS theme distributed
        as prebuilt extension for the lab template)
        Default: 'light'
        Equivalent to: [--HTMLExporter.theme]
    --sanitize_html=<Bool>
        Whether the HTML in Markdown cells and cell outputs should be sanitized.This
        should be set to True by nbviewer or similar tools.
        Default: False
        Equivalent to: [--HTMLExporter.sanitize_html]
    --writer=<DottedObjectName>
        Writer class used to write the
                                            results of the conversion
        Default: 'FilesWriter'
        Equivalent to: [--NbConvertApp.writer_class]
    --post=<DottedOrNone>
        PostProcessor class used to write the
                                            results of the conversion
        Default: ''
        Equivalent to: [--NbConvertApp.postprocessor_class]
    --output=<Unicode>
        overwrite base name use for output files.
                    can only be used when converting one notebook at a time.
        Default: ''
        Equivalent to: [--NbConvertApp.output_base]
    --output-dir=<Unicode>
        Directory to write output(s) to. Defaults
                                      to output to the directory of each notebook. To recover
                                      previous default behaviour (outputting to the current
                                      working directory) use . as the flag value.
        Default: ''
        Equivalent to: [--FilesWriter.build_directory]
    --reveal-prefix=<Unicode>
        The URL prefix for reveal.js (version 3.x).
                This defaults to the reveal CDN, but can be any url pointing to a copy
                of reveal.js.
                For speaker notes to work, this must be a relative path to a local
                copy of reveal.js: e.g., "reveal.js".
                If a relative path is given, it must be a subdirectory of the
                current directory (from which the server is run).
                See the usage documentation
                (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)
                for more details.
        Default: ''
        Equivalent to: [--SlidesExporter.reveal_url_prefix]
    --nbformat=<Enum>
        The nbformat version to write.
                Use this to downgrade notebooks.
        Choices: any of [1, 2, 3, 4]
        Default: 4
        Equivalent to: [--NotebookExporter.nbformat_version]

    Examples
    --------

        The simplest way to use nbconvert is

                > jupyter nbconvert mynotebook.ipynb --to html

                Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides', 'webpdf'].

                > jupyter nbconvert --to latex mynotebook.ipynb

                Both HTML and LaTeX support multiple output templates. LaTeX includes
                'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and
                'classic'. You can specify the flavor of the format used.

                > jupyter nbconvert --to html --template lab mynotebook.ipynb

                You can also pipe the output to stdout, rather than a file

                > jupyter nbconvert mynotebook.ipynb --stdout

                PDF is generated via latex

                > jupyter nbconvert mynotebook.ipynb --to pdf

                You can get (and serve) a Reveal.js-powered slideshow

                > jupyter nbconvert myslides.ipynb --to slides --post serve

                Multiple notebooks can be given at the command line in a couple of
                different ways:

                > jupyter nbconvert notebook*.ipynb
                > jupyter nbconvert notebook1.ipynb notebook2.ipynb

                or you can specify the notebooks list in a config file, containing::

                    c.NbConvertApp.notebooks = ["my_notebook.ipynb"]

                > jupyter nbconvert --config mycfg.py

    To see all available configurables, use `--help-all`.
:::
:::

::: {.cell .code execution_count="33" colab="{\"base_uri\":\"https://localhost:8080/\",\"height\":106}" id="lEOpbRQJG_mD" outputId="02d09717-4247-43bd-e735-1fee1a191c9a"}
``` python
jupyter nbconvert --to markdown RB_121450065.ipynb
```

::: {.output .error ename="SyntaxError" evalue="invalid syntax (<ipython-input-33-69d383fbd1a9>, line 1)"}
      File "<ipython-input-33-69d383fbd1a9>", line 1
        jupyter nbconvert --to markdown RB_121450065.ipynb
                ^
    SyntaxError: invalid syntax
:::
:::
